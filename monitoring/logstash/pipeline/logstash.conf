input {
  tcp {
    port => 5000
    codec => json_lines
  }
  
  udp {
    port => 5000
    codec => json_lines
  }
  
  # HTTP input for direct log shipping
  http {
    port => 8080
    codec => json
  }
}

filter {
  # Parse JSON logs
  if [message] =~ /^\{.*\}$/ {
    json {
      source => "message"
    }
  }
  
  # Extract timestamp
  if [timestamp] {
    date {
      match => [ "timestamp", "ISO8601", "yyyy-MM-dd HH:mm:ss.SSS" ]
      target => "@timestamp"
    }
  }
  
  # Add service metadata
  if [service] {
    mutate {
      add_field => { "[@metadata][service]" => "%{service}" }
    }
  }
  
  # Parse correlation ID
  if [correlationId] {
    mutate {
      add_field => { "trace_id" => "%{correlationId}" }
    }
  }
  
  # Parse log level
  if [level] {
    mutate {
      lowercase => [ "level" ]
      add_field => { "log_level" => "%{level}" }
    }
  }
  
  # Extract exception information
  if [exception] {
    mutate {
      add_field => { "has_exception" => "true" }
    }
  }
  
  # Grok patterns for common log formats
  grok {
    match => {
      "message" => [
        "%{TIMESTAMP_ISO8601:timestamp} %{LOGLEVEL:level} \[%{DATA:service}\] \[%{DATA:thread}\] %{DATA:logger} - %{GREEDYDATA:log_message}",
        "%{TIMESTAMP_ISO8601:timestamp} %{LOGLEVEL:level} %{DATA:logger} - %{GREEDYDATA:log_message}"
      ]
    }
    overwrite => [ "message" ]
  }
  
  # Add environment tag
  mutate {
    add_field => { "environment" => "${ENVIRONMENT:development}" }
  }
  
  # Remove unnecessary fields
  mutate {
    remove_field => [ "host", "port" ]
  }
}

output {
  elasticsearch {
    hosts => ["elasticsearch:9200"]
    index => "bookstore-logs-%{+YYYY.MM.dd}"
    document_type => "_doc"
  }
  
  # Output to stdout for debugging
  stdout {
    codec => rubydebug
  }
}